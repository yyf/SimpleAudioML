{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d4fe86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step - loss: 3.1527\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step - loss: 3.1527\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 770.2202\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 770.2202\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 128.5314\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 128.5314\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 184.7474\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 184.7474\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 250.3753\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 250.3753\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 286.4419\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 286.4419\n",
      "Epoch 7/10\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 294.0249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 294.0249\n",
      "Epoch 8/10\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 285.9749\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 285.9749\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 271.6035\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 271.6035\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 253.9451\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 253.9451\n",
      "Generated Spectrogram Stats:\n",
      "Min: 0.4530636668205261, Max: 0.5408461093902588\n",
      "Contains NaN: False, Contains Inf: False\n",
      "Generated Spectrogram Stats:\n",
      "Min: 0.4530636668205261, Max: 0.5408461093902588\n",
      "Contains NaN: False, Contains Inf: False\n",
      "Audio successfully generated and saved.\n",
      "Audio successfully generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "def load_audio(file_path, sr=22050, duration=5, target_shape=(128, 128)):\n",
    "    audio, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=target_shape[1])\n",
    "    log_spectrogram = librosa.power_to_db(spectrogram).T\n",
    "\n",
    "    # Normalize spectrogram to [0, 1]\n",
    "    log_spectrogram = (log_spectrogram - np.min(log_spectrogram)) / (np.max(log_spectrogram) - np.min(log_spectrogram) + 1e-10)\n",
    "\n",
    "    # Ensure the spectrogram has the target shape\n",
    "    if log_spectrogram.shape[0] > target_shape[0]:\n",
    "        log_spectrogram = log_spectrogram[:target_shape[0], :]  # Crop\n",
    "    elif log_spectrogram.shape[0] < target_shape[0]:\n",
    "        padding = target_shape[0] - log_spectrogram.shape[0]\n",
    "        log_spectrogram = np.pad(log_spectrogram, ((0, padding), (0, 0)), mode='constant')  # Pad\n",
    "\n",
    "    return log_spectrogram\n",
    "\n",
    "# Define the VAE\n",
    "latent_dim = 16\n",
    "\n",
    "# Encoder\n",
    "def build_encoder(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Flatten()(inputs)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    return tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Decoder\n",
    "def build_decoder(output_shape):\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(np.prod(output_shape), activation='sigmoid')(x)\n",
    "    outputs = layers.Reshape(output_shape)(x)\n",
    "    return tf.keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
    "\n",
    "# Update the VAE loss function to work with symbolic tensors\n",
    "class VAELossLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        inputs, outputs, z_mean, z_log_var = inputs\n",
    "\n",
    "        # Stabilize reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(inputs - outputs))  # Mean Squared Error\n",
    "\n",
    "        # Stabilize KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + tf.clip_by_value(z_log_var, -10.0, 10.0) - tf.square(z_mean) - tf.exp(tf.clip_by_value(z_log_var, -10.0, 10.0)),\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        # Add a small epsilon to avoid NaN\n",
    "        total_loss = reconstruction_loss + tf.reduce_mean(kl_loss + 1e-10)\n",
    "        self.add_loss(total_loss)\n",
    "        return outputs\n",
    "\n",
    "# Build and compile the VAE\n",
    "input_shape = (128, 128)  # Example spectrogram shape\n",
    "encoder = build_encoder(input_shape)\n",
    "decoder = build_decoder(input_shape)\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "z_mean, z_log_var, z = encoder(inputs)\n",
    "outputs = decoder(z)\n",
    "\n",
    "# Add the custom loss layer\n",
    "vae_outputs = VAELossLayer()([inputs, outputs, z_mean, z_log_var])\n",
    "vae = tf.keras.Model(inputs, vae_outputs, name=\"vae\")\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Load data and train\n",
    "file_path = \"../data/raw/07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.wav\"\n",
    "spectrogram = load_audio(file_path)\n",
    "spectrogram = np.expand_dims(spectrogram, axis=0)  # Add batch dimension\n",
    "\n",
    "vae.fit(spectrogram, spectrogram, epochs=10, batch_size=1)\n",
    "\n",
    "# Generate new audio\n",
    "latent_sample = tf.random.normal(shape=(1, latent_dim))\n",
    "generated_spectrogram = decoder(latent_sample).numpy().squeeze()\n",
    "\n",
    "# Debugging the generated spectrogram\n",
    "print(\"Generated Spectrogram Stats:\")\n",
    "print(f\"Min: {np.min(generated_spectrogram)}, Max: {np.max(generated_spectrogram)}\")\n",
    "print(f\"Contains NaN: {np.isnan(generated_spectrogram).any()}, Contains Inf: {np.isinf(generated_spectrogram).any()}\")\n",
    "\n",
    "# Ensure the spectrogram has valid values\n",
    "generated_spectrogram = np.nan_to_num(generated_spectrogram, nan=1e-10, posinf=1e-10, neginf=1e-10)\n",
    "generated_spectrogram = np.clip(generated_spectrogram, a_min=1e-10, a_max=None)  # Avoid log(0) issues\n",
    "\n",
    "# Convert spectrogram back to audio\n",
    "try:\n",
    "    generated_audio = librosa.feature.inverse.mel_to_audio(librosa.db_to_power(generated_spectrogram.T))\n",
    "    sf.write(\"../output/generated_audio_VAE.wav\", generated_audio, samplerate=22050)\n",
    "    print(\"Audio successfully generated and saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during audio generation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
